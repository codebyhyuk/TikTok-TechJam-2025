{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>business_name</th>\n",
       "      <th>business_category</th>\n",
       "      <th>business_description</th>\n",
       "      <th>_id</th>\n",
       "      <th>policy_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>4</td>\n",
       "      <td>Very clean place and great customer service. P...</td>\n",
       "      <td>Da Hawaiian Poke Company</td>\n",
       "      <td>['Hawaiian restaurant']</td>\n",
       "      <td>Classic poke is made with fresh local fish at ...</td>\n",
       "      <td>1.1646785119946047e+20_1485149267122</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>1</td>\n",
       "      <td>Numerous better options on the island, there's...</td>\n",
       "      <td>Da Hawaiian Poke Company</td>\n",
       "      <td>['Hawaiian restaurant']</td>\n",
       "      <td>Classic poke is made with fresh local fish at ...</td>\n",
       "      <td>1.0267678641614776e+20_1491101534705</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>5</td>\n",
       "      <td>We visited this place for macadamia nuts and o...</td>\n",
       "      <td>Da Hawaiian Poke Company</td>\n",
       "      <td>['Hawaiian restaurant']</td>\n",
       "      <td>Classic poke is made with fresh local fish at ...</td>\n",
       "      <td>1.0332983715774515e+20_1530623117395</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>4</td>\n",
       "      <td>Fresh ahi poke made fresh everyday.  Depending...</td>\n",
       "      <td>Da Hawaiian Poke Company</td>\n",
       "      <td>['Hawaiian restaurant']</td>\n",
       "      <td>Classic poke is made with fresh local fish at ...</td>\n",
       "      <td>1.132809466479012e+20_1498982367483</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>4</td>\n",
       "      <td>They offer a variety of fresh poke in their di...</td>\n",
       "      <td>Da Hawaiian Poke Company</td>\n",
       "      <td>['Hawaiian restaurant']</td>\n",
       "      <td>Classic poke is made with fresh local fish at ...</td>\n",
       "      <td>1.0883490360769959e+20_1459418469416</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  rating                                               text  \\\n",
       "0       10000       4  Very clean place and great customer service. P...   \n",
       "1       10001       1  Numerous better options on the island, there's...   \n",
       "2       10002       5  We visited this place for macadamia nuts and o...   \n",
       "3       10003       4  Fresh ahi poke made fresh everyday.  Depending...   \n",
       "4       10004       4  They offer a variety of fresh poke in their di...   \n",
       "\n",
       "              business_name        business_category  \\\n",
       "0  Da Hawaiian Poke Company  ['Hawaiian restaurant']   \n",
       "1  Da Hawaiian Poke Company  ['Hawaiian restaurant']   \n",
       "2  Da Hawaiian Poke Company  ['Hawaiian restaurant']   \n",
       "3  Da Hawaiian Poke Company  ['Hawaiian restaurant']   \n",
       "4  Da Hawaiian Poke Company  ['Hawaiian restaurant']   \n",
       "\n",
       "                                business_description  \\\n",
       "0  Classic poke is made with fresh local fish at ...   \n",
       "1  Classic poke is made with fresh local fish at ...   \n",
       "2  Classic poke is made with fresh local fish at ...   \n",
       "3  Classic poke is made with fresh local fish at ...   \n",
       "4  Classic poke is made with fresh local fish at ...   \n",
       "\n",
       "                                    _id  policy_label  \n",
       "0  1.1646785119946047e+20_1485149267122           1.0  \n",
       "1  1.0267678641614776e+20_1491101534705           0.0  \n",
       "2  1.0332983715774515e+20_1530623117395           1.0  \n",
       "3   1.132809466479012e+20_1498982367483           1.0  \n",
       "4  1.0883490360769959e+20_1459418469416           1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/data_gpt_labeler/labeled_datasets/final_data_labeled_2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_label\n",
      "1.0    5167\n",
      "0.0     805\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['policy_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yumin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/yumin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.149825\n",
      "1       0.368621\n",
      "2       0.044459\n",
      "3       0.820201\n",
      "4       0.326972\n",
      "          ...   \n",
      "9995    0.015148\n",
      "9996    0.107810\n",
      "9997    0.009407\n",
      "9998    0.011269\n",
      "9999    0.008957\n",
      "Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from modules.policy_A_module import policy_A_feature_generation\n",
    "feature_A = policy_A_feature_generation(df)\n",
    "print(feature_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yumin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/yumin/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/yumin/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from modules.policy_B_module import calculate_specificity_score\n",
    "feature_B = calculate_specificity_score(df, n_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from modules.policy_D2_module import calculate_interpretability_scores_for_df\n",
    "feature_D2 = calculate_interpretability_scores_for_df(df, n_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17060) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17061) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17062) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17063) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17064) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17065) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17066) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17067) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from modules.policy_E_module import compute_consistency_scores\n",
    "feature_E = compute_consistency_scores(df, max_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17434) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17435) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Exception in initializer:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yumin/opt/anaconda3/lib/python3.12/concurrent/futures/process.py\", line 242, in _process_worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/feature_engineering_model/modules/policy_G_module.py\", line 19, in init_worker\n",
      "    get_detoxify_model(model_name_for_worker)\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/feature_engineering_model/modules/policy_G_module.py\", line 14, in get_detoxify_model\n",
      "    _MODEL = Detoxify(model_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/detoxify/detoxify.py\", line 104, in __init__\n",
      "    self.model, self.tokenizer, self.class_names = load_checkpoint(\n",
      "                                                   ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/detoxify/detoxify.py\", line 41, in load_checkpoint\n",
      "    loaded = torch.hub.load_state_dict_from_url(checkpoint_path, map_location=device)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/torch/hub.py\", line 875, in load_state_dict_from_url\n",
      "    return torch.load(cached_file, map_location=map_location, weights_only=weights_only)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/torch/serialization.py\", line 1491, in load\n",
      "    with _open_zipfile_reader(opened_file) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/torch/serialization.py\", line 771, in __init__\n",
      "    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\n",
      "Exception in initializer:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yumin/opt/anaconda3/lib/python3.12/concurrent/futures/process.py\", line 242, in _process_worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/feature_engineering_model/modules/policy_G_module.py\", line 19, in init_worker\n",
      "    get_detoxify_model(model_name_for_worker)\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/feature_engineering_model/modules/policy_G_module.py\", line 14, in get_detoxify_model\n",
      "    _MODEL = Detoxify(model_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/detoxify/detoxify.py\", line 104, in __init__\n",
      "    self.model, self.tokenizer, self.class_names = load_checkpoint(\n",
      "                                                   ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/detoxify/detoxify.py\", line 41, in load_checkpoint\n",
      "    loaded = torch.hub.load_state_dict_from_url(checkpoint_path, map_location=device)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/torch/hub.py\", line 875, in load_state_dict_from_url\n",
      "    return torch.load(cached_file, map_location=map_location, weights_only=weights_only)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/torch/serialization.py\", line 1491, in load\n",
      "    with _open_zipfile_reader(opened_file) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/venv/lib/python3.12/site-packages/torch/serialization.py\", line 771, in __init__\n",
      "    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenProcessPool\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodules\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolicy_G_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_policy_g_series_processed\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m feature_G = \u001b[43mcompute_policy_g_series_processed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TikTok-TechJam-2025/feature_engineering_model/modules/policy_G_module.py:89\u001b[39m, in \u001b[36mcompute_policy_g_series_processed\u001b[39m\u001b[34m(df, text_col, model_name, max_workers, field)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_policy_g_series_processed\u001b[39m(\n\u001b[32m     83\u001b[39m     df: pd.DataFrame,\n\u001b[32m     84\u001b[39m     text_col: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     field: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mS_toxicity\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# keep default consistent with docstring\u001b[39;00m\n\u001b[32m     88\u001b[39m ) -> pd.Series:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     details = \u001b[43mcompute_policy_g_details_processed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m field \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m details.columns:\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mField \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not produced by score_toxicity().\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/TikTok-TechJam-2025/feature_engineering_model/modules/policy_G_module.py:76\u001b[39m, in \u001b[36mcompute_policy_g_details_processed\u001b[39m\u001b[34m(df, text_col, model_name, max_workers)\u001b[39m\n\u001b[32m     74\u001b[39m futures = [ex.submit(_policy_g_worker, (i, t, model_name)) \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts)]\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     idx, res = \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols:\n\u001b[32m     78\u001b[39m         out[c][idx] = \u001b[38;5;28mfloat\u001b[39m(res.get(c, \u001b[32m0.0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mBrokenProcessPool\u001b[39m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "from modules.policy_G_module import compute_policy_g_series_processed\n",
    "feature_G = compute_policy_g_series_processed(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat Data into New CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "# df = pd.concat([feature_A, feature_B, feature_D2, feature_E, feature_G], axis=1)\n",
    "df = pd.concat([feature_A, feature_B, feature_D2, feature_E], axis=1)\n",
    "# df = df.rename(columns={0:\"A\", 1:\"B\", 2: \"D2\", 3:\"E\", 4:\"G\"})\n",
    "df = df.rename(columns={0:\"A\", 1:\"B\", 2: \"D2\", 3:\"E\"})\n",
    "df.to_csv(\"featured_datasets/final_data_featured_2.csv\") #change file name so it does not overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data (this should have been done before feature extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       Unnamed: 0  rating                                               text  \\\n",
      "0          10000       4  Very clean place and great customer service. P...   \n",
      "1          10001       1  Numerous better options on the island, there's...   \n",
      "2          10002       5  We visited this place for macadamia nuts and o...   \n",
      "3          10003       4  Fresh ahi poke made fresh everyday.  Depending...   \n",
      "4          10004       4  They offer a variety of fresh poke in their di...   \n",
      "...          ...     ...                                                ...   \n",
      "9979       19979       4  Awesome experience thus far except for this la...   \n",
      "9980       19980       5                Excellent food and customer service   \n",
      "9994       19994       4            Enjoyed most of the food...good service   \n",
      "9998       19998       4  The Luau salad is excellent! Pasta Da Vinci wa...   \n",
      "9999       19999       5  Lunch was great fast to get seated and food wa...   \n",
      "\n",
      "                 business_name  \\\n",
      "0     Da Hawaiian Poke Company   \n",
      "1     Da Hawaiian Poke Company   \n",
      "2     Da Hawaiian Poke Company   \n",
      "3     Da Hawaiian Poke Company   \n",
      "4     Da Hawaiian Poke Company   \n",
      "...                        ...   \n",
      "9979    The Cheesecake Factory   \n",
      "9980    The Cheesecake Factory   \n",
      "9994    The Cheesecake Factory   \n",
      "9998    The Cheesecake Factory   \n",
      "9999    The Cheesecake Factory   \n",
      "\n",
      "                                      business_category  \\\n",
      "0                               ['Hawaiian restaurant']   \n",
      "1                               ['Hawaiian restaurant']   \n",
      "2                               ['Hawaiian restaurant']   \n",
      "3                               ['Hawaiian restaurant']   \n",
      "4                               ['Hawaiian restaurant']   \n",
      "...                                                 ...   \n",
      "9979  ['Restaurant', 'American restaurant', 'Dessert...   \n",
      "9980  ['Restaurant', 'American restaurant', 'Dessert...   \n",
      "9994  ['Restaurant', 'American restaurant', 'Dessert...   \n",
      "9998  ['Restaurant', 'American restaurant', 'Dessert...   \n",
      "9999  ['Restaurant', 'American restaurant', 'Dessert...   \n",
      "\n",
      "                                   business_description  \\\n",
      "0     Classic poke is made with fresh local fish at ...   \n",
      "1     Classic poke is made with fresh local fish at ...   \n",
      "2     Classic poke is made with fresh local fish at ...   \n",
      "3     Classic poke is made with fresh local fish at ...   \n",
      "4     Classic poke is made with fresh local fish at ...   \n",
      "...                                                 ...   \n",
      "9979  American chain restaurant offering sizable por...   \n",
      "9980  American chain restaurant offering sizable por...   \n",
      "9994  American chain restaurant offering sizable por...   \n",
      "9998  American chain restaurant offering sizable por...   \n",
      "9999  American chain restaurant offering sizable por...   \n",
      "\n",
      "                                       _id  policy_label  \n",
      "0     1.1646785119946047e+20_1485149267122           1.0  \n",
      "1     1.0267678641614776e+20_1491101534705           0.0  \n",
      "2     1.0332983715774515e+20_1530623117395           1.0  \n",
      "3      1.132809466479012e+20_1498982367483           1.0  \n",
      "4     1.0883490360769959e+20_1459418469416           1.0  \n",
      "...                                    ...           ...  \n",
      "9979  1.1063547377321743e+20_1529799075601           1.0  \n",
      "9980   1.148709745904846e+20_1552799908686           1.0  \n",
      "9994  1.1485002627657445e+20_1536125465743           1.0  \n",
      "9998  1.0947861878968879e+20_1528365104726           1.0  \n",
      "9999   1.012667000112821e+20_1536711337614           1.0  \n",
      "\n",
      "[5972 rows x 8 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/yumin/Documents/GitHub/TikTok-TechJam-2025/data_gpt_labeler/labeled_datasets/final_data_labeled_2.csv')\n",
    "df_filtered = df[df['policy_label'].notna()]\n",
    "df_filtered.to_csv(\"featured_datasets/final_data_featured_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
